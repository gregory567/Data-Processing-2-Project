{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis of Pfizer vaccine tweets Worldwide\n",
    "\n",
    "**Original Author:** Elena Stamatelou.<br/>\n",
    "**Additional Info:** Sentiment analysis on streaming twitter data using Spark Structured Streaming & Python. https://github.com/stamatelou/twitter_sentiment_analysis<br/>\n",
    "**Last Modified:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the os module \n",
    "import os\n",
    "\n",
    "# Set the PYSPARK_SUBMIT_ARGS to the appropriate spark-sql-kafka package\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: textblob in /opt/conda/lib/python3.8/site-packages (0.15.3)\n",
      "Requirement already satisfied, skipping upgrade: nltk>=3.1 in /opt/conda/lib/python3.8/site-packages (from textblob) (3.6.2)\n",
      "Requirement already satisfied, skipping upgrade: regex in /opt/conda/lib/python3.8/site-packages (from nltk>=3.1->textblob) (2021.4.4)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /opt/conda/lib/python3.8/site-packages (from nltk>=3.1->textblob) (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /opt/conda/lib/python3.8/site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk>=3.1->textblob) (4.51.0)\n"
     ]
    }
   ],
   "source": [
    "# Install textblob for the sentiment analysis\n",
    "import sys\n",
    "!{sys.executable} -m pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Sparksession and sql functions \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text classification\n",
    "\n",
    "# Define methods from TextBlob\n",
    "def polarity_detection(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def subjectivity_detection(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "def sentiment_detection(value):\n",
    "    if value < 0: \n",
    "        return 'Negative'\n",
    "    elif value > 0: \n",
    "        return 'Positive'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# polarity detection\n",
    "# Define as user defined fuction to embed method in the spark environment \n",
    "polarity_detection_udf = udf(polarity_detection, StringType())\n",
    "\n",
    "# subjectivity detection\n",
    "# Define as user defined fuction to embed method in the spark environment \n",
    "subjectivity_detection_udf = udf(subjectivity_detection, StringType())\n",
    "\n",
    "# sentiment detection\n",
    "# Define as user defined fuction to embed method in the spark environment \n",
    "sentiment_detection_udf = udf(sentiment_detection, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the findspark module \n",
    "import findspark\n",
    "\n",
    "# Initialize via the full spark path\n",
    "findspark.init(\"/usr/local/spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local[*]\") \\\n",
    "   .appName(\"TwitterSentAnalysis\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    # Read Tweets from the Kafka topic vaccine \n",
    "    tweet_df = spark.readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "        .option(\"subscribe\", \"pfizer_worldwide\") \\\n",
    "        .option(\"startingOffsets\", \"latest\") \\\n",
    "        .load()\n",
    "except: \n",
    "    print(\"Unexpected error:\", sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- streaming is running -------\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    # Cast the data into a json\n",
    "    tweet_df_string = tweet_df.selectExpr(\"CAST(value AS STRING) as json_data\")\n",
    "    \n",
    "    # extract the tweet and user info\n",
    "    text_user = tweet_df_string.select(json_tuple('json_data', 'created_at','text', 'user').alias('created_at', 'text', 'json_user')) \n",
    "    \n",
    "    # extract screen_name and location from user info\n",
    "    text_user_info = text_user.select('text', 'created_at', json_tuple('json_user', 'location').alias('location')) \n",
    "    \n",
    "    # preprocessing\n",
    "    text_user_info = text_user_info.na.replace('', 'None')\n",
    "    text_user_info = text_user_info.na.drop()\n",
    "    \n",
    "    text_user_info = text_user_info.withColumn('text', F.regexp_replace('text', r'http\\S+', ''))\n",
    "    text_user_info = text_user_info.withColumn('text', F.regexp_replace('text', '@\\w+', ''))\n",
    "    text_user_info = text_user_info.withColumn('text', F.regexp_replace('text', '#', ''))\n",
    "    text_user_info = text_user_info.withColumn('text', F.regexp_replace('text', 'RT', ''))\n",
    "    text_user_info = text_user_info.withColumn('text', F.regexp_replace('text', ':', ''))\n",
    "\n",
    "    # polarity detection\n",
    "    # Append polarity to dataframe\n",
    "    text_user_info = text_user_info.withColumn(\"polarity\", polarity_detection_udf(text_user_info.text))\n",
    "    \n",
    "    # subjectivity detection\n",
    "    # Append subjectivity to dataframe \n",
    "    text_user_info = text_user_info.withColumn(\"subjectivity\", subjectivity_detection_udf(text_user_info.text))\n",
    "    \n",
    "    #sentiment detection\n",
    "    # Append sentiment to dataframe\n",
    "    text_user_info = text_user_info.withColumn(\"sentiment\", sentiment_detection_udf(text_user_info.polarity))\n",
    "    \n",
    "    # repartition 'Returns a new DataFrame partitioned by the given partitioning expressions. The resulting DataFrame is hash partitioned.'\n",
    "    text_user_info = text_user_info.repartition(1)\n",
    "    \n",
    "    # country filter -> no filter since worldwide \n",
    "    \n",
    "    #option(\"header\", \"True\"). \\\n",
    "    #Write the spark stream\n",
    "    writeTweet = text_user_info.writeStream. \\\n",
    "        format(\"csv\"). \\\n",
    "        option(\"checkpointLocation\", \"./storage_pfizer_worldwide/\"). \\\n",
    "        option(\"path\", \"./storage_pfizer_worldwide/\"). \\\n",
    "        outputMode(\"append\"). \\\n",
    "        queryName(\"worldwide_pfizer_tweets\"). \\\n",
    "        trigger(processingTime='60 seconds'). \\\n",
    "        start()\n",
    "    \n",
    "    print(\"----- streaming is running -------\")\n",
    "    \n",
    "    writeTweet.awaitTermination()\n",
    "    \n",
    "except: \n",
    "    print(\"Unexpected error:\", sys.exc_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
